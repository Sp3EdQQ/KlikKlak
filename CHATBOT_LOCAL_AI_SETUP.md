# Konfiguracja Lokalnego AI dla Chatbota

## ðŸŽ¯ Cel

MoÅ¼liwoÅ›Ä‡ pÅ‚ynnego przeÅ‚Ä…czania miÄ™dzy lokalnym modelem AI a OpenAI (GPT-4o) dla:
- **Lokalnie**: Testy i rozwÃ³j bez kosztÃ³w
- **Produkcja**: OpenAI GPT-4o dla lepszej jakoÅ›ci

## ðŸ”§ Konfiguracja

### Zmienne Å›rodowiskowe (`.env`)

```env
# PrzeÅ‚Ä…czanie miÄ™dzy lokalnym AI a OpenAI
USE_LOCAL_AI=true                    # true = lokalny AI, false = OpenAI

# Konfiguracja lokalnego AI
LOCAL_AI_URL=http://localhost:11434/v1
LOCAL_AI_MODEL=llama3.2

# Konfiguracja OpenAI (dla produkcji)
OPENAI_API_KEY=sk-proj-your-key-here
OPENAI_MODEL=gpt-4o
```

## ðŸ“¦ Rekomendowane lokalne AI

### 1. **Ollama** (Najprostsze, Zalecane) â­

```bash
# Instalacja (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Lub rÄ™czne pobranie
# https://ollama.ai/download

# Uruchom model
ollama pull llama3.2
ollama serve

# Model dostÄ™pny na: http://localhost:11434
```

**Konfiguracja w `.env`:**
```env
USE_LOCAL_AI=true
LOCAL_AI_URL=http://localhost:11434/v1
LOCAL_AI_MODEL=llama3.2
```

**Rekomendowane modele:**
- `llama3.2` (8GB RAM) - Dobry poczÄ…tek
- `llama3.1` (16GB RAM) - Lepsza jakoÅ›Ä‡
- `mistral` (8GB RAM) - Alternatywa

### 2. **LM Studio** (GUI, Å‚atwe w uÅ¼yciu)

1. Pobierz: https://lmstudio.ai/
2. Zainstaluj model (np. Llama 3, Mistral)
3. Uruchom serwer lokalny (zakÅ‚adka "Local Server")
4. DomyÅ›lny port: `1234`

**Konfiguracja w `.env`:**
```env
USE_LOCAL_AI=true
LOCAL_AI_URL=http://localhost:1234/v1
LOCAL_AI_MODEL=llama-3.2-8b
```

### 3. **LocalAI** (Samodzielny serwer)

```bash
# Docker
docker run -p 8080:8080 localai/localai:latest

# Lub z docker-compose
```

**Konfiguracja w `.env`:**
```env
USE_LOCAL_AI=true
LOCAL_AI_URL=http://localhost:8080/v1
LOCAL_AI_MODEL=your-model-name
```

## ðŸš€ Szybki Start

### Opcja 1: Lokalne testy (Ollama)

```bash
# 1. Zainstaluj Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pobierz model
ollama pull llama3.2

# 3. Uruchom serwer Ollama (w osobnym terminalu)
ollama serve

# 4. Skonfiguruj backend
cd backend
cat > .env << EOF
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/klikklak
JWT_SECRET=dev-secret
JWT_EXPIRES_IN=1h
JWT_REFRESH_SECRET=dev-refresh-secret
JWT_REFRESH_EXPIRES_IN=7d

# LOKALNY AI
USE_LOCAL_AI=true
LOCAL_AI_URL=http://localhost:11434/v1
LOCAL_AI_MODEL=llama3.2

PORT=3000
NODE_ENV=development
CORS_ORIGIN=http://localhost:5173
EOF

# 5. Uruchom backend
pnpm install
pnpm run start:dev
```

### Opcja 2: Produkcja (OpenAI)

```bash
# Skonfiguruj backend z OpenAI
cd backend
cat > .env << EOF
DATABASE_URL=postgresql://postgres:postgres@production-db:5432/klikklak
JWT_SECRET=strong-production-secret
JWT_EXPIRES_IN=1h
JWT_REFRESH_SECRET=strong-refresh-secret
JWT_REFRESH_EXPIRES_IN=7d

# OPENAI
USE_LOCAL_AI=false
OPENAI_API_KEY=sk-proj-YOUR-REAL-KEY-HERE
OPENAI_MODEL=gpt-4o

PORT=3000
NODE_ENV=production
CORS_ORIGIN=https://yourdomain.com
EOF

# Uruchom backend
pnpm install
pnpm run build
pnpm run start:prod
```

## ðŸ”„ PrzeÅ‚Ä…czanie miÄ™dzy Å›rodowiskami

### Z lokalnego AI na OpenAI:

```bash
# W pliku .env zmieÅ„:
USE_LOCAL_AI=false
OPENAI_API_KEY=sk-proj-your-key-here
OPENAI_MODEL=gpt-4o
```

### Z OpenAI na lokalny AI:

```bash
# W pliku .env zmieÅ„:
USE_LOCAL_AI=true
LOCAL_AI_URL=http://localhost:11434/v1
LOCAL_AI_MODEL=llama3.2
```

**Restart backendu:** Wystarczy zrestartowaÄ‡ aplikacjÄ™ - zmiana zostanie automatycznie zaÅ‚adowana.

## ðŸ§ª Testowanie

### Test lokalnego API:

```bash
# SprawdÅº czy Ollama dziaÅ‚a
curl http://localhost:11434/api/version

# Test prostego zapytania
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### Test chatbota w aplikacji:

```bash
# WyÅ›lij zapytanie do chatbota
curl http://localhost:3000/chatbot/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "PokaÅ¼ mi procesory Intel do 1500 zÅ‚",
    "conversationHistory": []
  }'
```

## âš™ï¸ KompatybilnoÅ›Ä‡ z Function Calling

**WAÅ»NE:** Nie wszystkie lokalne modele wspierajÄ… Function Calling (Tools API).

### Modele z peÅ‚nym wsparciem Function Calling:
- âœ… **Llama 3.1 i 3.2** (8B+) - Bardzo dobre wsparcie
- âœ… **Mistral 7B v0.3+** - Dobre wsparcie
- âœ… **Hermes 2 Pro** - Specjalnie trenowany do function calling

### Modele z ograniczonym wsparciem:
- âš ï¸ **Llama 3.0** - CzÄ™Å›ciowe wsparcie
- âš ï¸ **Starsze modele** - MogÄ… nie dziaÅ‚aÄ‡ poprawnie

### RozwiÄ…zania problemÃ³w:

JeÅ›li lokalne AI nie obsÅ‚uguje function calling:
1. **ZmieÅ„ na model z peÅ‚nym wsparciem** (Llama 3.2)
2. **UÅ¼yj OpenAI do testÃ³w funkcji** (ustaw `USE_LOCAL_AI=false`)
3. **Dostosuj prompty** - niektÃ³re modele wymagajÄ… innych formatÃ³w

## ðŸ’° PorÃ³wnanie kosztÃ³w

### Lokalny AI:
- âœ… **Koszt:** 0 zÅ‚
- âœ… **PrywatnoÅ›Ä‡:** Dane nie opuszczajÄ… serwera
- âš ï¸ **WydajnoÅ›Ä‡:** ZaleÅ¼y od sprzÄ™tu
- âš ï¸ **JakoÅ›Ä‡:** NiÅ¼sza niÅ¼ GPT-4o

### OpenAI GPT-4o:
- âš ï¸ **Koszt:** ~$5 za 1M tokenÃ³w wejÅ›ciowych
- âš ï¸ **PrywatnoÅ›Ä‡:** Dane wysyÅ‚ane do OpenAI
- âœ… **WydajnoÅ›Ä‡:** Bardzo szybkie
- âœ… **JakoÅ›Ä‡:** NajwyÅ¼sza dostÄ™pna

### Rekomendacja:
- **RozwÃ³j i testy:** Lokalny AI (Ollama + Llama 3.2)
- **Produkcja:** OpenAI GPT-4o

## ðŸ” Monitorowanie

Backend automatycznie loguje informacje o uÅ¼ywanym AI:

```
[ChatbotService] Using LOCAL AI: http://localhost:11434/v1 with model: llama3.2
```

lub

```
[ChatbotService] Using OpenAI with model: gpt-4o
```

## ðŸ› RozwiÄ…zywanie problemÃ³w

### Problem: "Connection refused" do lokalnego AI

```bash
# SprawdÅº czy serwer dziaÅ‚a
curl http://localhost:11434/api/version

# JeÅ›li nie dziaÅ‚a, uruchom ponownie
ollama serve
```

### Problem: Model nie wspiera function calling

ZmieÅ„ model na nowszy:
```bash
ollama pull llama3.2
# Zaktualizuj LOCAL_AI_MODEL=llama3.2
```

### Problem: Backend nie Å‚Ä…czy siÄ™ z AI

SprawdÅº logi backendu:
```bash
pnpm run start:dev
# Szukaj "Using LOCAL AI" lub "Using OpenAI"
```

### Problem: SÅ‚aba jakoÅ›Ä‡ odpowiedzi z lokalnego modelu

1. UÅ¼yj wiÄ™kszego/lepszego modelu:
   ```bash
   ollama pull llama3.1:70b  # Wymaga wiÄ™cej RAM
   ```
2. Lub przeÅ‚Ä…cz siÄ™ tymczasowo na OpenAI:
   ```env
   USE_LOCAL_AI=false
   ```

## ðŸ“š Dodatkowe zasoby

- Ollama: https://ollama.ai/
- LM Studio: https://lmstudio.ai/
- LocalAI: https://github.com/mudler/LocalAI
- Lista modeli: https://ollama.ai/library

## ðŸŽ“ Najlepsze praktyki

1. **Lokalnie testuj z maÅ‚ymi modelami** (Llama 3.2 8B)
2. **Na produkcji uÅ¼yj OpenAI** dla najlepszej jakoÅ›ci
3. **Zawsze testuj function calling** przed wdroÅ¼eniem
4. **Monitoruj logi** aby upewniÄ‡ siÄ™ ktÃ³ry AI jest uÅ¼ywany
5. **Dodaj .env do .gitignore** - nie commituj kluczy API
